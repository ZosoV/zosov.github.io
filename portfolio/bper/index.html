
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Bisimulation Prioritized Experience Replay</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\\(', '\\)']] } });
      </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
 
    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <!-- <meta property="og:image" content="assets/CovNet_teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://jonbarron.info/mipnerf/"/>
    <meta property="og:title" content="mip-NeRF" />
    <meta property="og:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." /> -->

        <!--TWITTER-->
    <!-- <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="mip-NeRF" />
    <meta name="twitter:description" content="Project page for Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields." />
    <meta name="twitter:image" content="https://jonbarron.info/mipnerf/img/rays_square.png" /> -->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
	<link rel="icon" href="../../assets/icons/icon-192x192.png">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">

    <script src="js/app.js"></script>
</head>

<body>
    <a href="https://zosov.github.io/" class="fixed-button">
        <i class="fas fa-home"></i>
    </a>
    
    <div class="container-fluid" id="main">
        <div class="row" id="header_project">
            <h2 class="col-md-12 text-center">
                <b>Bisimulation Prioritized Experience Replay</b>: <br> Enhancing Online Reinforcement Learning through <br>Behavioral-Based Priorities <br>
                <small>
                    Master Thesis 2023-2024
                </small>
            </h2>
        </div>
        <div class="row" id="header_project">
            <div class="col-md-12 text-center">
                <ul class="list-inline" >
                    <li>
                        <a href="https://zosov.github.io/">
                            Oscar Guarnizo
                        </a>
                        </br>University of Birmingham
                    </li>
                    <li>
                        <a href="https://mircogiacobbe.github.io/">
                          Mirco Giacobbe
                        </a>
                        </br>University of Birmingham
                    </li><br>
                    <li>
                        <a href="https://www.leonardostella.com/">
                            Leonardo Stella
                        </a>
                        </br>University of Birmingham
                    </li>
                </ul>
            </div>
        </div>


        <div class="row" id="header_project">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <!-- <li>
                            <a href="https://ieeexplore.ieee.org/document/9416639">
                            <image src="assets/mip_paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li> -->
                        <li>
                            <a href="#">
                            <image src="assets/mip_paper_image.jpg" height="60px">
                                <h4><strong>Thesis</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="#">
                            <image src="assets/mip_paper_image.jpg" height="60px">
                                <h4><strong>Poster Article</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://docs.google.com/presentation/d/1JLb2d-s9r5cLp-ThYM192pepzLsk0x6jFZRSWZSLA2Q/edit?usp=sharing">
                            <image src="assets/google slides.png" height="60px">
                                <h4><strong>Demo Slides</strong></h4>
                            </a>
                        </li>    
                        <!-- <li>
                            <a href="https://www.researchgate.net/publication/361880577_Poster_Convolutional_Neural_Network_Feature_Extraction_Using_Covariance_Tensor_Decomposition">
                            <image src="assets/poster.png" height="60px">
                                <h4><strong>Poster</strong></h4>
                            </a>
                        </li>                         -->
                        <li>
                            <a href="https://youtu.be/ajT4eAypdJM">
                            <image src="assets/youtube_icon.png" height="60px">
                                <h4><strong>State Abstractions Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/ZosoV/final_project">
                            <image src="assets/github.png" height="60px" style="filter: invert(1);">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                        <!-- <li>
                            <a href="https://oscar-guarnizo.medium.com/review-covariance-tensor-for-convolutional-neural-networks-77c119f2a225">
                            <image src="assets/medium.png" height="60px" style="filter: invert(1);">
                                <h4><strong>Blog</strong></h4>
                            </a>
                        </li> -->
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="assets/latent_space.jpg" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Prioritized Experience Replay has been an effective traditional solution for 
                    value-based reinforcement learning algorithms to efficiently address 
                    non-stationary and correlated data issues. However, standard prioritization 
                    often overlooks the nuanced, task-specific behaviors of states, leading to a 
                    "task-agnostic" sampling problem. This work introduces a novel non-uniform 
                    sampling approach, named Bisimulation Prioritized Experience Replay (BPER), 
                    by incorporating a surrogate on-policy bisimulation metric into the experience 
                    replay prioritization process. This metric allows us to measure behavioral 
                    similarities and diversify the training data, aiming to enhance learning by 
                    focusing on behaviorally relevant transitions. Specifically, our method utilizes 
                    a Matching under Independent Couplings (MICo) metric, a more general surrogate metric 
                    learned through state abstractions. The proposed method balances conventional 
                    TD-error-based and bisimulation-based prioritization by reweighting priorities 
                    with an introduced hyperparameter, and two possible strategies to assigning 
                    priorities. The method demonstrates superior performance in a 31-state Grid 
                    World and shows promising results in classical pixel-based environments. The 
                    31-state Grid World empirically validates the proof of concept by efficiently 
                    achieving to 1) emphasize behavioral relevant transition, thereby avoiding task-agnostic 
                    sampling, 2) alleviate the outdated priorities by having a better tendency to constant 
                    fixed priorities, and 3) mitigate the insufficient sample space coverage, increasing 
                    the data diversity.             
                </p>
            </div>
        </div>

        <!-- <p>
            NOTE: Content TBD waiting thesis released.
        </p> -->

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="https://www.youtube.com/embed/0vH1tGhuZbY" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
            </div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Matching under Independent Couplings (MICo) Metric <a href="https://arxiv.org/abs/2106.08229">(Castro et al., 2022)</a>
                </h3>
                <p class="text-justify">
                    The present work employs a surrogate on-policy bisimulation metric known as the MICO metric <a href="https://arxiv.org/abs/2106.08229">(Castro et al., 2022)</a>,
                    which confers a behavioral similarity measure between states. The MICO metric is a more general metric learned through state abstractions, which works for both
                    deterministic and stochastic MDPs. 
                </p>
                <p style="text-align:center;">
                    <image src="assets/mico_definition.png" class="img-responsive">
                </p>
                <p>
                    It is important to notice that the MICo metric is a diffuse metric, which allows zero values for distinct states, and self-distances greater than zero.
                </p>
                <p style="text-align:center;"></p>
                    <image src="assets/diffuse_metric.png" class="img-responsive">
                </p>                
                <p>
                    This MICo operator is used to learn a parameterized state abstraction \(\phi_\omega(x) : \mathcal{S} \to \mathcal{S}_\phi\) (parameterized by \(\omega\)), which maps a true environmental state (e.g., pixels) to a lower-dimensional latent representation. 
                    The goal is to position these representations in such a way that a chosen parameterized distance $U_\omega(x,y)$ 
                    coincides with the MICo distance in the latent space, where the states are organized according to their behavioral similarity.
                </p>
                <p style="text-align:center;">
                    <image src="assets/latent_space.jpg" class="img-responsive">
                </p>
                <p class="text-justify">
                    The parameterized distance function \(U_\omega(x, y)\) is defined as:
                    \begin{equation}
                        U^\pi(x, y) \approx U_\omega(x, y):=\frac{\left\|\phi_\omega(x)\right\|_2^2+\left\|\phi_\omega(y)\right\|_2^2}{2}+\beta \theta\left(\phi_\omega(x), \phi_\omega(y)\right)
                    \end{equation}
                    where the first term ensures positive self-distances.</p>

                <p>
                    Consequently, the recursive nature of the operator \(T_M^\pi(U)(x,y)\) is used to define a loss function, which works similarly to the Bellman recurrence process in DQN.
                    Specifically, the loss function measures the difference between a learning target MICo metric \(T_{\bar{\omega}}^U\) and the online MICo metric \(U_\omega\), as

                    \begin{equation}
                    \label{eq:mico_loss}
                        \mathcal{L}_{\mathrm{MICo}}(\omega)=\mathbb{E}_{\left\langle x, r_x, x^{\prime}\right\rangle,\left\langle y, r_y, y^{\prime}\right\rangle}\left[\left(T_{\bar{\omega}}^U\left(r_x, x^{\prime}, r_y, y^{\prime}\right)-U_\omega(x, y)\right)^2\right]
                    \end{equation}

                    \begin{equation}
                        T_{\bar{\omega}}^U\left(r_x, x^{\prime}, r_y, y^{\prime}\right)=\left|r_x-r_y\right|+\gamma U_{\bar{\omega}}\left(x^{\prime}, y^{\prime}\right)
                    \end{equation}
                    where \(\bar{\omega}\) is a separate copy of the network parameters, synchronized with \(\omega\) at infrequent intervals, and the pairs of transitions \(\left\langle x, r_x, x^{\prime}\right\rangle\) and \(\left\langle y, r_y, y^{\prime}\right\rangle\) are sampled from the experience replay.
                </p>

                <p>The MICO learning can be integrated into any value-based agent by learning an estimate $Q_{\xi, \omega}(x, \cdot) = \psi_\xi(\phi_\omega(x))$, where $\phi_\omega(x)$ corresponds to the representation of state $x$, and $\psi_\xi$ corresponds to the value approximator. This work specifically focuses on the DQN algorithm, where the $Q_{\xi, \omega}$ corresponds to the Q-values. In this scenario, the MICO loss $\mathcal{L}_{\text{TD}}$ is combined with the temporal-difference loss $\mathcal{L}_{\text{MICo}}$ as 
                    \begin{equation}
                        \mathcal{L}_\alpha(\xi, \omega) = (1 - \alpha)\mathcal{L}_{\text{TD}}(\xi, \omega) + \alpha \mathcal{L}_{\text{MICo}}(\omega)
                    \end{equation}
                    , where $\alpha \in (0, 1)$. 
                </p>

                <p style="text-align:center;">
                    <image src="assets/mico_learning.jpg"  class="img-responsive">
                </p>

                <p>
                    Although the MICo loss $\mathcal{L}_{\text{MICo}}$ requires pairs of transitions $\left\langle x, r_x, x^{\prime}\right\rangle$ and $\left\langle y, r_y, y^{\prime}\right\rangle$, in practice, transitions are not sampled as pairs; we only have access to a mini-batch of unpaired transitions, necessitating a method to pair them. To address this issue, a <b>squarify</b> method <a href="https://arxiv.org/abs/1911.09291">(Castro, 2019)</a> was proposed to construct pairs of transitions by pairing each transition with all others within the current mini-batch and then calculating the MICo loss.
                </p>
                <p style="text-align:center;">
                    <image src="assets/squarify.jpg"  class="img-responsive">
                </p>
            </div>
        </div>
            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Bisimulation Prioritized Experience Replay (BPER)
                </h3>
                <p class="text-justify">
                    The current work explores the behaviors between states stored in the experience replay. Note that an experience tuple in an experience replay is
                    defined as \(e_t = (s_t, a_t, r_t, s_{t+1})\). It is evident that experiences with a higher MICO on-policy bisimulation (more behavioral dissimilar)
                    distance between \(s_t\) and \(s_{t+1}\) are likely to be more informative (or 'surprising') than transitions between
                    behaviorally similar states. By prioritizing transitions with greater behavioral differences, our method encourages more
                    diversity in the sampling process.

                    As the MICo metric is approximated online as part of the state abstraction learning process, it is enough to use
                    $U_\omega$ to calculate a MICo distance between the current and next states and update the priorities accordingly. This is our <b>current-next Strategy (BPERcn)</b>.
                </p>
                <p style="text-align:center;">
                    <image src="assets/strategies.jpg" class="img-responsive" alt="scales">
                </p>
    
                <p>
                The current-next Strategy faces challenges due to, in practice, the trajectories may overlap significantly, resulting in overlapping state distributions, leading to scarce high-priority transitions. 
                An empirical solution is proposed to mitigate these issues and improve prioritization effectiveness. In this <b>all-vs-all Strategy (BPERaa)</b>, a relative behavioral distance to the current minibatch is 
                computed by averaging the mean distance between each current state and all other current states in the minibatch; computed as
                \begin{equation}
                U^B_\omega(s_i) = \sum_{i=1}^k U_\omega(s_i, s_k), \quad \forall e_k = (s_k, r_k, a_k, s_{k+1}) \in B
            \end{equation}
            where $B$ corresponds to the current mini-batch.         
                <br>
                <br>
                Algorithm 3 presents the pseudo-code for the entire prioritizing process. Similar to the PER method, 
                the priorities are only updated for the current sampled mini-batch to maintain computational efficiency.
                Additionally, alpha-weighting the probability and importance sampling are used similarly to PER. 
                Notice the algorithm depicts the procedure for current-next Strategy (BPERcn). all-vs-all Strategy 
                (BPERaa) requires a minor replacement in line 19, where \(U_\omega(s_i, s_{i+1})\) should be 
                replaced with the relative distance \(U^B_\omega(s_i)\).
                
   
                </p>
                <p style="text-align:center;">
                    <image src="assets/algorithm.png" class="img-responsive" alt="scales">
                </p>
            </div>
        </div>
            


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <p class="text-justify">
                    A 31-state simple GridWorld was used as a proof of concept to evaluate three key problems in non-uniform sampling
                    methods: task-agnostic sampling, outdated priorities, and state space coverage. After that, a general evaluation was performed on classical environments. 
                    
                    <br>
                    <br>
                    <b>Task-agnostic Sampling.</b>
                    The following plot illustrates the evolution of priority distributions in the experience replay over time, with the final distribution (at 100k steps) highlighted at the top.
                    The results show that both proposed strategies, BPERcn and BPERaa, consistently generate higher priority values over time, encouraging more behaviorally
                    dissimilar transitions, while PER exhibits an accumulation of low priority values over time. Notably, BPERaa exhibits slightly greater variability in the priority values.

                </p>
                <p style="text-align:center; align-items: center;">
                    <image src="assets/priority_distributions.jpg" class="img-responsive" alt="scales">
                </p>
                <p class="text-justify">
                    <b>Outdated Priorities.</b>
                    Recall that outdated priorities occur because priority updates are performed only on the current minibatch, rather than
                    across the whole possible transition under the current policy.
                    <br>
                    <br>
                    The following plots show the distances between the sampling distribution $p_i$ for PER and BPER variants, and the ideal
                    distributions $p^\ast_i$, achieved when all possible transitions under the current policy are updated at each time step.
                    Two weighting schemes, the on-policy and uniform weighting, were used based on the previous work of <a href="https://arxiv.org/abs/2007.09569">Pan et al., 2022</a>.
                    Although our method does not provides distances close to zero indicating a perfect match between distributions, our
                    method reduce and alleviate the difference between the distributions without relying on a model-based techniques.

                </p>
                <p style="text-align:center; align-items: center;">
                    <image src="assets/outdated_priorities.jpg" class="img-responsive" alt="scales">
                </p>
                <p class="text-justify">
                    <b>Space Coverage.</b>
                    Recall the state space coverage problem arises from insufficient exploration, where the experience replay only captures
                    a limited number of possible states, leading to suboptimal learning outcomes.
                    <br>
                    <br>
                    Figure bellow shows the state visitation distribution at the 90k time step in the 31-state
                    Grid World, where cell values corresponds to the visit count per state. The results indicate that the strategies BPERcn
                    and BPERaa visit more states more frequently compared to other methods, with BPERcn performing slightly better than
                    BPERaa. While some of the increased exploration in BPERcn and BPERaa may be attributed to the MICo learning, the
                    bisimulation prioritized technique further enhances exploration (relative to DQN + MICo) by significantly increasing the
                    state visit counts, reaching values around 600.
                </p>                             
                <p style="text-align:center; align-items: center;">
                    <image src="assets/space_coverage.jpg" class="img-responsive" alt="scales">
                </p>
                <p class="text-justify">
                    <b>Classical Environments.</b>
                    Figures bellow illustrate the episode reward calculated over a moving average window of 100 episodes in four different
                    pixel-based environments, averaged over 5 independent executions. The hyperparameter priority weight $\mu$ was set using
                    the best values found. In these results, the bisimulation strategies outperform the direct baseline DQN + MICo in most
                    of the environments, even if they do not surpass DQN or DQN + PER.
                </p>                             
                <p style="text-align:center; align-items: center;">
                    <image src="assets/classical_environments.jpg" class="img-responsive" alt="scales">
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Related links
                </h3>
                <p class="text-justify">
                    - The on-policy bisimulation metric was proposed on the paper <a href="https://arxiv.org/abs/1911.09291">Scalable methods for computing state similarity in deterministic Markov Decision Processes</a> by Castro (2020).
                </p>
                <p class="text-justify">
                    - The MICO paper can be found on <a href="https://arxiv.org/abs/2106.08229">MICo: Improved representations via sampling-based state similarity for Markov decision processes</a> by Castro et al. (2022).
                </p>
                <p class="text-justify">
                    - A new alternative for calculating the MICo metric using kernels is proposed on <a href="https://arxiv.org/abs/2310.19804">A Kernel Perspective on Behavioural Metrics for Markov Decision Processes</a> by Castro et al. (2023).
                </p>

            </div>
        </div>
        
            
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@article{fonseca2021convolutional,
    title={Convolutional Neural Network Feature Extraction Using Covariance Tensor Decomposition},
    author={Fonseca, Ricardo and Guarnizo, Oscar and Suntaxi, Diego and Cadiz, Alfonso and Creixell, Werner},
    journal={IEEE Access},
    volume={9},
    pages={66646--66660},
    year={2021},
    publisher={IEEE}
}</textarea>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p style="text-align: right">
                Powered by <a href="https://jonbarron.info/">Jon Barron</a> and <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>
