<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Note-taking Page</title>
    <!-- Link to the external style sheet -->
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\\(', '\\)']] } });
      </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <script src="app.js" defer></script>


</head>
<body>
    <nav>
        <!-- TODO: Change this when you create other project -->
        <a href="../../uob_first_semester/index.html"><i class="fas fa-home"></i></a>
        <a href="#" id="prevLink"><i class="fas fa-arrow-left"></i></a>
        <a href="#" id="nextLink"><i class="fas fa-arrow-right"></i></a>
    
        <div class="dropdown">
          <a href="#"><i class="fas fa-caret-down"></i></a>
          <div class="dropdown-content" id="dropdownContent">
            <!-- Links will be dynamically added here using JavaScript -->
          </div>
        </div>
    </nav>

    <div class="center-container">
        <h1>Is Learning Feasible?: Generalization Bound</h1>
    </div>

    <!--  MAIN BLOCK NOTE CONTAINER -->
    <div class="note-container">
        <div class="topic">
            <p> Generalization Bound Idea  </p>
            <!-- Add your title content here -->
        </div>
        <div class="content">
            <p> 
                A ==Generalization bound== help us to understand how the Generalization
                Error $E_{out}$ could be, even though we cannot compute it. <br><br>
                In order to understand that, the bound is defined in terms of the
                training error $E_{in}$, which we can compute. <br> <br>

                Starting from ==Hoeffding Inequality==, we can derive two possible
                escenarios:
                <ol>
                    <li>==Maximum Bound==: What is the **maximum** possible value for $E_{out}$</li>
                    <li>==Minimum Bound==: What is the **minimum** possible value for $E_{out}$</li>
                </ol>

                The first one is of more interest because it could explain how well
                our $E_{in}$ can estimate $E_{out}$ by providing a small maximum bound. The following image depicts this derivation
            </p>
    
            <div class="img-container">
                <img src="assets/rewrite_hoeff.jpg" alt="test">
            </div>
        </div>

        <div class="comment">
          <!-- Add your comment content here -->
            <p> A Generalization Bound is other way to understand if 
                Learning is feasible or not, we will came up to similar conclusions
                as with Hoeffding Inequality.
            </p>
        </div>
    </div>
    <!--  END BLOCK NOTE CONTAINER -->

    <!--  MAIN BLOCK NOTE CONTAINER -->
    <div class="note-container">
        <div class="topic">
            <!-- Add your title content here -->
        </div>
        <div class="content">
            <p>
                In order to have a better expression of $\epsilon$, we use
                assume that the right term of Hoeffding Inequality is a term $\delta$
                that we, as users, defined beforehand.                 
            </p>
            <div class="small-img-container">
                <img src="assets/delta.png" alt="test">
            </div>
            <p>
                and we can isolate $\epsilon$ to get

                $$\epsilon = \sqrt{\frac{1}{2N}\text{ln}\left(\frac{2M}{\delta}\right)}$$

                Notice that by replacing $\epsilon$ in any of our two previous cases
                we have an expression in terms of $N$ (==training set size==) 
                and $M$ (==model complexity==). <br><br>
                Now we, as users, will define $\delta$ instead of $\epsilon$.
            </p>

            <p>
                In such way, we say that starting from the Hoeffding Inequality with probability ==$1 - \delta$==,
                the following bounds are true 

                ==Maximum Bound==

                $$E_{out}(g) \leq E_{in}(g) + \sqrt{\frac{1}{2N}\text{ln}\left(\frac{2M}{\delta}\right)}$$

                == Minimal Bound==

                $$E_{out}(g) \geq E_{in}(g) - \sqrt{\frac{1}{2N}\text{ln}\left(\frac{2M}{\delta}\right)}$$
            </p>

        </div>

        <div class="comment">
          <!-- Add your comment content here -->
            <p>Notice the isolation $\epsilon$ from $\delta$ is
            </p>
            <div class="img-container">
                <img src="assets/delta.png" alt="test">
            </div>
            <p>
                $$\text{ln}\left(\frac{\delta}{2M}\right) = -2N\epsilon^2$$
                $$\epsilon^2 = - \frac{1}{2N} \text{ln}\left(\frac{\delta}{2M}\right)$$
                $$\epsilon = \sqrt{\frac{1}{2N}ln\left(\frac{2M}{\delta}\right)}$$
            </p>
        </div>
    </div>
    <!--  END BLOCK NOTE CONTAINER -->

        <!--  MAIN BLOCK NOTE CONTAINER -->
        <div class="note-container">
            <div class="topic">
               <p>Maximum Bound (First Case)</p> 
            </div>
            <div class="content">
                <p>
                    The following is the definition of the maximum bound
                
                    $$E_{out}(g) \leq E_{in}(g) + \sqrt{\frac{1}{2N}\text{ln}\left(\frac{2M}{\delta}\right)}$$

                    Notice we can conclude
                </p>
                    <ol>
                        <li>$\downarrow E_{in} \quad \downarrow E_{out} \Rightarrow$ 
                            limits the generalization error by a smaller training error $E_{in}$</li>
                        <li>$\uparrow N \quad \downarrow E_{out} \Rightarrow$ more training size less generalization error</li>
                        <li>$\uparrow M \quad \uparrow E_{out} \Rightarrow$ more model complexity more generalization</li>
                    </ol>

                <div class="callout">
                    <p >    
                        **__Trade-off M, $E_{in}$, $E_{out}$__**
                        <br>
    
                        Additionally, we can note again that there exists a complex relation
                        between $M$, $E_{in}$, and $E_{out}$ similar to what happened with the Uniform Hoeffnig Inequality 
                    </p>
                    <p >
                        By focusing in the complex relation that exists between $M$, $E_{in}$, and $E_{out}$,
                        we can split the learning in two central questions:
            
                        <div class="img-container">
                          <img src="assets/trade_off1.jpg" alt="test">
                        </div>
                        <ol>
                          <li>
                            can we make sure that $E_{out}(g)$ is close enough to $E_{in}(g)$
                          </li>
                          <li>
                            can we make $E_{in}(g)$ small enough?
                          </li>
                        </ol>
                      
                      
                      <div class="img-container">
                        <img src="assets/trade_off.jpg" alt="test">
                      </div>
            
                      Then, using the right $M$ (or $\mathcal{H}$) is important.
                        </p>
                </div>

                        <!-- (TODO: I could add a link) -->

            </div>
    
            <div class="comment">
              <!-- Add your comment content here -->
                <p>Notice that although the generalization error increases with
                    the model complexity, it doesn't increase too much due to the
                    square root and logarithm. <br><br>

                    --Regularization can account for this trade-off by minimizing $E_{in}$ and at the same time trying to minimize the complexity of the model M--
                </p>

            </div>
        </div>
        <!--  END BLOCK NOTE CONTAINER -->

        <!--  MAIN BLOCK NOTE CONTAINER -->
        <div class="note-container">
            <div class="topic">
                <p>Minimum Bound (Second Case)</p>
                
            </div>
            <div class="content">
                <p>The following is the definition of the minimum bound
                
                    $$E_{out}(g) \geq E_{in}(g) - \sqrt{\frac{1}{2N}\text{ln}\left(\frac{2M}{\delta}\right)}$$

                    Notice we can conclude $\uparrow E_{in} \quad \uparrow E_{out}$. <br><br>
                    
                    Then, this bound effectively provides a kind of
                    minimum bound that our generalization error can achieve. It can not necesarily
                    be an absolute zero generalization error. It is limited again by the $E_{in}$
                </p>
    
            </div>
    
            <div class="comment">
              <!-- Add your comment content here -->

            </div>
        </div>
        <!--  END BLOCK NOTE CONTAINER -->

                <!--  MAIN BLOCK NOTE CONTAINER -->
                <div class="note-container">
                    <div class="topic">
                        <p>Finete Hypothesis Set Assumption</p>
                        
                    </div>
                    <div class="content">
                        <p>Notice the whole derivation is possible because we assume
                            a finite set $H$. However, our ML algorithm usually have 
                            infinite hypothesis sets. 
                        </p>
            
                    </div>
            
                    <div class="comment">
                      <!-- Add your comment content here -->
        
                    </div>
                </div>
                <!--  END BLOCK NOTE CONTAINER -->
</body>
</html>
